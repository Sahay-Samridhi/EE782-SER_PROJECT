# EE782-SER_PROJECT
Comparison of LSTM, Transformer (wav2vec2.0 and HuBERT) and CNN (VGGish) based methods for speech emotion recognition
Speech Emotion Recognition (SER): A Comparative Analysis of Deep Learning Models & Loss Functions
# üìå Project Overview
This repository contains the implementation of a comprehensive comparative study on Speech Emotion Recognition (SER). The project evaluates the performance of four distinct feature extractors‚ÄîLSTM, VGGish, Wav2Vec 2.0, and HuBERT‚Äîpaired with five different loss functions to determine the optimal combinations for capturing emotional cues in speech1.All experiments were conducted on the benchmark IEMOCAP dataset, focusing on four emotion categories: anger, happiness, sadness, and neutral
# üìÇ Repository Structure
The code is organized by model architecture. For Wav2Vec 2.0, specific notebooks are provided for each loss function investigated.Core Model ImplementationsFileDescriptionHuBERT.ipynbImplementation of the HuBERT (Hidden-Unit BERT) model. In our study, this model achieved the highest overall accuracy (81.41%) when paired with Focal Loss3.LSTM.ipynbImplementation of the LSTM (Long Short-Term Memory) network. This notebook explores sequential modeling of acoustic features4.VGGish.ipynbImplementation of the VGGish CNN architecture, pre-trained on AudioSet, treating audio spectrograms as image inputs5555.Wav2Vec 2.0 Loss Function ExperimentsThese notebooks contain the Wav2Vec 2.0 implementation fine-tuned with specific objective functions6:FileLoss Function Focuswav2vec2_cross_entropy.ipynbStandard Categorical Cross-Entropy (CE), serving as the robust baseline7.wav2vec2_focal_loss.ipynbFocal Loss implementation to address class imbalance and hard-to-classify samples8.wav2vec2_label_smoothening_ce.ipynbLabel Smoothing, used to prevent model overconfidence and improve generalization9.wav2vec2_AAM_loss.ipynbAdditive Angular Margin (AAM) loss, designed to enhance feature discriminability in the embedding space10.wav2vec2_CCC_loss.ipynbConcordance Correlation Coefficient (CCC) loss, adapted to maximize agreement between predicted and actual emotion ratings11.üìä Experimental ResultsThe following table summarizes the accuracy (%) achieved by each model-loss combination on the IEMOCAP dataset12:ModelCross-EntropyLabel SmoothingFocal LossAAMCCCLSTM56.1056.3752.6647.3458.23VGGish57.5457.0056.1927.7355.65Wav2Vec 2.074.4672.1772.2961.3374.22HuBERT81.3080.0981.4178.6663.80Key Findings:Best Overall: HuBERT + Focal Loss achieved the state-of-the-art accuracy of 81.41%13.Architecture Dependence: Transformer-based models (HuBERT, Wav2Vec 2.0) significantly outperformed traditional LSTM and VGGish architectures14.Loss Function Synergy: While Cross-Entropy is a strong baseline, CCC Loss proved most effective for the sequential LSTM model, whereas Focal Loss maximized the potential of the HuBERT transformer15.‚öôÔ∏è PrerequisitesPython 3.xPyTorchHuggingFace Transformers (for HuBERT and Wav2Vec 2.0) 16Librosa (for audio processing)IEMOCAP Dataset (Access must be requested from USC)üë• ContributorsVirti Rohit MehtaSamridhi SahaySaumya Aryan
